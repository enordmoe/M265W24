---
title: "Chapters 18 and 19: Pipes and Functions"
subtitle: "MATH 265"
author: "Eric Nordmoe"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css","scrollingbox.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE, cache = TRUE)
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(forcats)
library(lubridate)
library(rvest)
#library(backports)
```



# Agenda

* The pipe

* Introduction to Functions  

* Investigation: Applications to scraping

---

## The Household Pipe: GIGO? 

![](https://i.pinimg.com/originals/58/75/e4/5875e4a7c38483336fafdd31062e0cb2.png)
---

## Magritte's Pipe 


![](https://upload.wikimedia.org/wikipedia/en/b/b9/MagrittePipe.jpg)

<div class="footnote">
Image taken from a University of Alabama site, "Approaches to Modernism": [1], Fair use, https://en.wikipedia.org/w/index.php?curid=555365
</div>

---
##  The Tidyverse Pipe  

![](figures/magrittr_pipe.jpeg)

---

## Why we need the pipe?  Violent rabbits??  

```{r, eval = FALSE}
foo_foo_1 <- hop(foo_foo, through = forest)
foo_foo_2 <- scoop(foo_foo_1, up = field_mice)
foo_foo_3 <- bop(foo_foo_2, on = head)
```

Stringing commands without the pipe leads to:

* Code cluttered with unimportant names  
    + But it's not about the use of memory  
* Need to keep track of object versions  
    + Sequential numbering like `lakers2`?  

---

## Overwriting names is problematic 

```{r, eval = FALSE}
foo_foo <- hop(foo_foo, through = forest)
foo_foo <- scoop(foo_foo, up = field_mice)
foo_foo <- bop(foo_foo, on = head)
```

* Need to rerun from the top when debugging  

* Code is hard to follow due to repetition    
    + What's changing each time?  

---
## Using the pipe! %>% 

```{r, eval = FALSE}
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mice) %>%
  bop(on = head) -> 
  the_output  
```

* Easy to follow  
* Focuses on actions  
* Avoids clutter and repitition  

---
## The Hadley-approved Version 

```{r, eval = FALSE}
# Easy to comment out the first line when debugging
the_output<- 
  foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mice) %>%
  bop(on = head)
```

* Easy to follow  
* Focuses on actions  
* Avoids clutter and repitition  

---
## When to avoid the pipe?  

* Many steps that cannot be condensed further.   

* Multiple inputs/outputs (but you might want to learn about the tee "%T>%")

* Non-linear process


---
class: inverse, center, middle

# Functions  

---
## From Calculus  

* A function is a rule  

* For every input we can only get one output  

* Passes vertical line test  

```{r, echo = F, fig.height = 4, warning = FALSE}
x <- seq(-2, 6, .05)
y <- x^3 - 6*x^2 + 4*x + 12
plot(x, y, 
     type="l", 
     xaxt='n', yaxt='n', 
     bty='n', 
     xlab='', ylab='', 
     col="blue")
##  Next add in your axis arrows:
arrows(min(x), 0, max(x), 0, lwd=1, length=0.15)
arrows(0, min(y), 0, max(y), lwd=1, length=0.15)
##  And plot your x/y labels.  Note that if you want them
##    actually at the end of the arrows you would need to 
##    remove the pos= argument and shorten your arrows by
##    a small amount.  To match your original figure, you can
##    alter the x/y coordinate to be the max() instead.
text(0, max(y), "f(x)", pos=2)
text(max(x), 0, "x", pos=3)
#axis(1)
#axis(2)
```

---

## Functions in Computer Science

```{r, out.width = '30%', fig.align='center', fig.cap='',echo=FALSE}
knitr::include_graphics('figures/Function_machine2.svg.png')
```


---
## Why Functions?  

```{r}
#  x^2 - 5x - 14 = 0 
(-(-5) +c(-1,1)*sqrt((-5)^2 - 4*1*(-14)))/( 2*1) 
```

```{r}
# 2x^2 + 2x − 12 = 0
(-(2) +c(-1,1)*sqrt((2)^2 - 4*(2)*(-12)))/( 2*2) 
```

```{r}
#  x^2 - 4x + 4 = 0 
(-(-4) +c(-1,1)*sqrt((-4)^2 - 4*1*(4)))/( 2*1) 
```


---

## Write a Famous Function

```{r}
quad_roots <- function(aval, bval, cval){
  (-bval +c(-1,1)*sqrt(bval^2 - 4*aval*cval))/( 2*aval) 
}
quad_roots(1, -5, -14)  
quad_roots(2, 2, -12)  
quad_roots(1, -4, 4)  # Gives repeated roots
```

---

## What goes in/What comes out?

* Functions take input(s) defined in the function definition

```{r, eval = FALSE}
function([inputs separated by commas]){
  # what to do with those inputs
}
```

* By default they return the last value computed in the function

* You can also define more outputs to be returned in a list as well as nice print methods


---
class: inverse, center, middle

# Functions and Automation: Scotland First Minister's COVID speeches

---


## Goal

Use to scrape all the first minister's speeches that contain COVID in the title and create a dataset of speeches containing the following information about the speeches:

1. Title

2. Date

3. Location

4. Abstract

5. Text of the speech

6. URL address of the speech

---

## Website Table of Contents 

![](figs/first_min_contents.png)

---

## The Goal

![](figs/first_min_speeches_df.png)

---

## Getting Oriented

#### [https://www.gov.scot/collections/first-ministers-speeches/](https://www.gov.scot/collections/first-ministers-speeches/)


![](figs/fm_speeches_p1.png)
---

![:scale 80%](figs/fm_speeches_p2.png)


---
## Plan

1. Scrape `title`, `date`, `location`, `abstract`, and `text` from a few COVID-19 speech pages to develop the code

2. Write a function that scrapes `title`, `date`, `location`, `abstract`, and `text` from COVID-19 speech pages

3. Scrape the `url`s of COVID-19 speeches from the main page

4. Use this function to scrape from each individual COVID-19 speech from these `url`s and create a data frame with the columns `title`, `date`, `location`, `abstract`, `text`, and `url`

---
class:  middle

## 1. Scrape data from a few COVID-19 speech pages

---
background-image: url("figs/fm_speeches_picture_oct20.png")
background-size: 25%
background-position: 90% 90%

## Read page for  October 26, 2020 speech

```{r, read-page}
# url <- "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-26-october/"
speech_page <- read_html("https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-26-october/")
url <- "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-26-october/"
```

```{r, eval= FALSE}
speech_page
```

---

## Extract title

Use Selector Gadget to find the appropriate node

```{r read-title}
title <- speech_page %>%
    html_node(".ds_page-header__title") %>%
    html_text()
title
```

---


## Extract date

* Use `dmy()` to format

```{r read-date}
library(lubridate)
date <- speech_page %>%
    html_node("#sg-meta__publication-date") %>%
    html_text() %>%
  dmy()
date
```


---

## Extract location


```{r}
location <- speech_page %>%
    html_node(".ds_metadata__item:nth-child(5) strong") %>%
    html_text() 
location
```



---

## Extract abstract


```{r}
abstract <- speech_page %>%
    html_node(".ds_no-margin--bottom") %>%
    html_text() 
abstract
```



---

## Extract text


```{r}
text <- speech_page %>%
    html_nodes("#preamble p") %>%
    html_text() %>%
  list()
text
```


---

## Put it all in a data frame

```{r}
oct_26_speech <- tibble(
  title    = title,
  date     = date,
  location = location,
  abstract = abstract,
  text     = text,
  url      = url
)
oct_26_speech
```

---

## Read page for October 23, 2020 speech

```{r}
url <- "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-23-october/"
speech_page <- read_html(url)
```

```{r}
speech_page
```

---

## Extract components of the October 23, 2020 speech

```{r}
title <- speech_page %>%
  html_node(".ds_page-header__title") %>%
  html_text()
date <- speech_page %>%
  html_node("#sg-meta__publication-date") %>%
  html_text() %>%
  dmy()
location <- speech_page %>%
  html_node(".ds_metadata__item:nth-child(5) strong") %>%
  html_text()
abstract <- speech_page %>%
  html_node(".ds_no-margin--bottom") %>%
  html_text()
text <- speech_page %>%
  html_nodes("#preamble p") %>%
  html_text() %>%
  list()
```

---

## Put it all in a data frame

```{r}
oct_23_speech <- tibble(
  title    = title,
  date     = date,
  location = location,
  abstract = abstract,
  text     = text,
  url      = url
)
oct_23_speech
```


---
class: inverse, center, middle

# Isn't there a way to speed this up?

---
class:  center, middle

# Functions to the Rescue

---

## When should you write a function?

* When you’ve copied and pasted a block of code more than twice.

---

### How many times would we need to copy and paste the code we developed to scrape data on all of First Minister's COVID-19 speeches?

![:scale 50%](figs/fm_speeches_p3.png)

---

## Why functions?
  
Automate common tasks in a more powerful and general way than copy-and-pasting:  
  
* Use names that indicate what the purpose of the function
   + Avoid reserved R names: like `c`, `sum`

* Make updating code easier allowing you to update in just one place instead of many

* Eliminate mistakes with copying and pasting
   + Forgetting to make corresponding changes in all places

* Learn to write and share your own functions!

---

### Assuming that the page structure is the same for each speech page, how many "things" do you need to know for each speech page to scrape the data we want from it?

.small-code[
```{r, eval = FALSE}
url <- "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-23-october/"
speech_page <- read_html(url)
title <- speech_page %>%
  html_node(".ds_page-header__title") %>%
  html_text()
date <- speech_page %>%
  html_node("#sg-meta__publication-date") %>%
  html_text() %>%
  dmy()
location <- speech_page %>%
  html_node(".ds_metadata__item:nth-child(5) strong") %>%
  html_text()
abstract <- speech_page %>%
  html_node(".ds_no-margin--bottom") %>%
  html_text()
text <- speech_page %>%
  html_nodes("#preamble p") %>%
  html_text() %>%
  list()
tibble(title = title, date  = date, location = location,  
       abstract = abstract, text = text,  url = url)
```
]

---

## Turn your code into a function

* Pick a short informative name, preferably a verb.

* List inputs, or **arguments**, to the function inside `function`. If we had more the call would look like `function(x, y, z)`.

* Place the code you have developed in body of the function, a `{` block that immediately follows `function(...)`.

```{r, eval = FALSE}
scrape_speech <- function(url){
  # code we developed earlier to scrape info 
  # on a single speech goes here
}
```

---
## Define the function scrape_speech()

.small-code[
```{r}
scrape_speech <- function(url) {
  speech_page <- read_html(url)
  title <- speech_page %>%
  html_node(".ds_page-header__title") %>%
  html_text()
date <- speech_page %>%
  html_node("#sg-meta__publication-date") %>%
  html_text() %>%
  dmy()
location <- speech_page %>%
  html_node(".ds_metadata__item:nth-child(5) strong") %>%
  html_text()
abstract <- speech_page %>%
  html_node(".ds_no-margin--bottom") %>%
  html_text()
text <- speech_page %>%
  html_nodes("#preamble p") %>%
  html_text() %>%
  list()
tibble(
  title = title, date = date, location = location,
  abstract = abstract, text = text, url = url
)
}
```
]

---

## Function scrape_speech() in action
### October 23, 2020

```{r}
scrape_speech(url = "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-23-october/") %>%
glimpse()
```

---

## Function scrape_speech() in action
### October 26, 2020

```{r}
scrape_speech(url = "https://www.gov.scot/publications/coronavirus-covid-19-update-first-ministers-speech-26-october/") %>%
glimpse()
```

---

## Automation?  
  
* We now have a function for scraping speech info given `url`?  

* How can we get a list of speech `url`s?
  
---

class:  center, middle

# Inputs

---

## All URLs

```{r, output.lines = 10}
all_speeches_page <- read_html("https://www.gov.scot/collections/first-ministers-speeches/")
all_speeches_page %>%
  html_nodes(".collections-list a") %>%
  html_attr("href") 
```


---

## COVID-19 URLs fragments

```{r , output.lines = 10}
all_speeches_page <- read_html("https://www.gov.scot/collections/first-ministers-speeches/")
all_speeches_page %>%
  html_nodes(".collections-list a") %>%
  html_attr("href") %>%
  str_subset("covid-19")
```

---
## Combine to get COVID-19 URLs

```{r , output.lines = 10}
all_speeches_page <- read_html("https://www.gov.scot/collections/first-ministers-speeches/")
all_speeches_page %>%
  html_nodes(".collections-list a") %>%
  html_attr("href") %>%
  str_subset("covid-19")%>%
  str_c("https://www.gov.scot", .)
```

---

## Save COVID-19 URLs

```{r, output.lines = 10}
covid_speech_urls <- all_speeches_page %>%
  html_nodes(".collections-list a") %>%
  html_attr("href") %>%
  str_subset("covid-19") %>%
  str_c("https://www.gov.scot", .)
covid_speech_urls
```

---

class:  center, middle

# Iteration

---

## Define the task

* Goal: Scrape info on all COVID-19 speeches of the First Minister
* So far:
.very-small-code[
```{r}
scrape_speech(covid_speech_urls[1])
```

```{r}
scrape_speech(covid_speech_urls[2])
```

```{r}
scrape_speech(covid_speech_urls[3])
```
]

---

## Now what?

* What else do we need to do?
  + Run the `scrape_speech()` function on all COVID-19 speech links
  + Combine the resulting data frames from each run into one giant data frame

---

## Iteration

* How can we tell R to apply the `scrape_speech()` function to each link in `covid_speech_urls`?

---
class: inverse, center, top

background-image: url(figures/purrr.png)
background-position: 50% 75%
background-size: 50% 50%

## We have the `purrr`fect solution! 

---
## Go to each page, scrape speech info 

* Map the `scrape_speech()` function
* to each element of `covid_speech_urls`
* and return a data frame by binding rows together

In other words, we want to **map** the `scrape_speech()` function to each element of `covid_speech_urls`:

```{r eval=FALSE}
covid_speeches <- map_df(covid_speech_urls, scrape_speech)
head(covid_speeches)
```


---

## Caution  

* If you get `HTTP Error 429 (Too many requests)` you might want to slow down your hits.

* You can add a `Sys.sleep()` call to slow down your function:

```{r eval=FALSE}
scrape_speech() <- function(x){
  Sys.sleep(runif(1))  #<<
  ...

}
```


.footnote[**IMPORTANT NOTE**: This exercise is intended to illustrate methods for using functions to obtain data from the web. For large scale automated processes, be sure to obtain permission from the web site you wish to scrape.]

---
# Acknowledgement  

Web scraping introduction to functions adapted from materials provided by [Data Science in a Box](https://datasciencebox.org/) at https://github.com/rstudio-education/datascience-box. 


